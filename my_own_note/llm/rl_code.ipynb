{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a766db",
   "metadata": {},
   "source": [
    "DPO\n",
    "\n",
    "$\\mathbb{E}_{x, y, y' \\sim D}\\log \\sigma (\\beta(\\log (\\frac{\\pi_{\\theta}(y', x)}{\\pi_{\\theta_{ref}}(y', x)} ) - \\log (\\frac{\\pi_{\\theta}(y, x)}{\\pi_{\\theta_{ref}}(y, x)})))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dpo_loss(new_logpro, old_logpro, new_ref_logpro, old_ref_logpro, beta=1.0):\n",
    "    loss = -beta * torch.log(torch.sigmoid(new_logpro - old_logpro + old_ref_logpro - new_ref_logpro))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8b4fa",
   "metadata": {},
   "source": [
    "PPO\n",
    "\n",
    "$r_i = \\frac{\\pi_{\\theta}(y_i, x_i)}{\\pi_{\\theta_{ref}}(y_i, x_i)} $\n",
    "\n",
    "$\\mathbb{E}_{i}[ \\min(r_i * A_i, clip(1 + \\epsilon, 1 - \\epsilon, r_i) * A_i)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5eb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(new_logpro, old_logpro, advantages, epsilon=0.2):\n",
    "    r = torch.exp(new_logpro-old_logpro)\n",
    "    loss = -torch.min(r * advantages, torch.clamp(r, min=1 - epsilon, max=1 + epsilon) * advantages)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234c231",
   "metadata": {},
   "source": [
    "GAE优势估计\n",
    "\n",
    "$\\delta_t = r_t + \\gamma(1-d_{t+1}) V_{s_{i+1}} - V_{s_{t}}$\n",
    "\n",
    "$A_{t} = \\delta_{t} + \\gamma \\lambda (1-d_{t+1}) A_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91790fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= GAE：单条轨迹版本（向量：长度 T）=========\n",
    "@torch.no_grad()\n",
    "def compute_gae_1d(rewards, values, dones, next_value, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    rewards: [T]\n",
    "    values:  [T]           V(s_t) (估计值)\n",
    "    dones:   [T]           终止标记(0/1)；用于(1 - done)作nonterminal\n",
    "    next_value: scalar     V(s_{T})\n",
    "    return: advantages[T], returns[T]\n",
    "    \"\"\"\n",
    "    T = rewards.shape[0]\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        nonterminal = 1.0 - dones[t]\n",
    "        next_val = next_value if t == T - 1 else values[t + 1]\n",
    "        delta = rewards[t] + gamma * next_val * nonterminal - values[t]\n",
    "        last_gae = delta + gamma * lam * nonterminal * last_gae\n",
    "        advantages[t] = last_gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# ========= GAE：时间-批次版本（矩阵：[T, B]）=========\n",
    "@torch.no_grad()\n",
    "def compute_gae_tb(rewards, values, dones, next_value, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    rewards:    [T, B]\n",
    "    values:     [T, B]\n",
    "    dones:      [T, B]     (0/1)\n",
    "    next_value: [B]        V(s_{T})\n",
    "    return: advantages[T, B], returns[T, B]\n",
    "    \"\"\"\n",
    "    T, B = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae = torch.zeros(B, device=rewards.device, dtype=rewards.dtype)\n",
    "    for t in reversed(range(T)):\n",
    "        nonterminal = 1.0 - dones[t]        # [B]\n",
    "        next_val = torch.where(\n",
    "            torch.tensor(t == T - 1, device=rewards.device),\n",
    "            next_value,\n",
    "            values[t + 1]\n",
    "        )                                   # [B]\n",
    "        delta = rewards[t] + gamma * next_val * nonterminal - values[t]\n",
    "        last_gae = delta + gamma * lam * nonterminal * last_gae\n",
    "        advantages[t] = last_gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# ========= PPO-Clip 策略损失 =========\n",
    "def ppo_clip_loss(new_logprobs, old_logprobs, advantages, eps=0.2):\n",
    "    \"\"\"\n",
    "    new_logprobs, old_logprobs, advantages: 形状一致（如 [N] 或 [T*B]）\n",
    "    \"\"\"\n",
    "    # 建议把 advantages detach & 标准化\n",
    "    adv = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
    "\n",
    "    r = torch.exp(new_logprobs - old_logprobs)       # [N]\n",
    "    surr1 = r * adv\n",
    "    surr2 = torch.clamp(r, 1 - eps, 1 + eps) * adv\n",
    "    # 最大化 clip 目标 => 最小化其相反数\n",
    "    loss_pi = -torch.min(surr1, surr2).mean()\n",
    "    return loss_pi\n",
    "\n",
    "\n",
    "# ========= 值函数损失（critic）=========\n",
    "def value_loss(values_pred, returns):\n",
    "    return 0.5 * F.mse_loss(values_pred, returns)\n",
    "\n",
    "\n",
    "# ========= 熵正则（离散动作示例：直接用策略分布的熵）=========\n",
    "def entropy_loss_from_logits(logits):\n",
    "    # logits: [N, A]\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    log_probs = (probs + 1e-8).log()\n",
    "    ent = -(probs * log_probs).sum(dim=-1)           # [N]\n",
    "    return -ent.mean()  # 作为“损失”项取负号；总损失里再减去 c_e * L_S 即可\n",
    "\n",
    "\n",
    "# ========= 一个整合的例子：给出本轮 PPO 的总损失 =========\n",
    "def ppo_total_loss(\n",
    "    policy_logits,              # [N, A] 新策略的logits\n",
    "    new_logprobs_taken,         # [N]    新策略在已选动作上的log prob\n",
    "    old_logprobs_taken,         # [N]    旧策略在已选动作上的log prob (rollout时缓存)\n",
    "    values_pred,                # [N]    新 value 预测 V(s_t)\n",
    "    returns,                    # [N]    GAE 回报目标\n",
    "    advantages,                 # [N]    GAE 优势\n",
    "    clip_eps=0.2,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.01\n",
    "):\n",
    "    loss_pi = ppo_clip_loss(new_logprobs_taken, old_logprobs_taken, advantages, eps=clip_eps)\n",
    "    loss_v  = value_loss(values_pred, returns)\n",
    "    loss_ent = entropy_loss_from_logits(policy_logits)  # 注意这个函数返回的是“负熵的均值”，已是损失形式\n",
    "    total = loss_pi + vf_coef * loss_v + ent_coef * loss_ent\n",
    "    return total, {\"pi\": loss_pi.item(), \"v\": loss_v.item(), \"ent\": loss_ent.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f5d4b",
   "metadata": {},
   "source": [
    "GRPO\n",
    "\n",
    "$r_t = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{ref}(a_t|s_t)}}$\n",
    "\n",
    "$a_t = \\frac{(a_t - \\mu_{group})}{\\sqrt(\\sigma_{group})}$\n",
    "\n",
    "$\\mathbb{E}_{t}(\\min(r_t * a_t, clap(1+\\epsilon, 1-\\epsilon, r_t) * a_t)) - D_{KL}(\\pi_{\\theta} || \\pi_{\\theta_{ref}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ff716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(rewards, new_logpro, old_logpro, epsilon=1e-5):\n",
    "    mean_r = rewards.mean(dim=-1, keepdim=True)\n",
    "    std_r = rewards.std(dim=-1, keepdim=True)\n",
    "    loss = torch.exp(new_logpro - old_logpro) * ((rewards - mean_r) / (std_r + epsilon))\n",
    "    kl_loss = -torch.mean(torch.exp(old_logpro) * (new_logpro - old_logpro))\n",
    "    return loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b65cd",
   "metadata": {},
   "source": [
    "GSPO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gspo_loss(rewards, new_logpro, old_logpro, epsilon=1e-5):\n",
    "    rewards = (rewards - rewards.mean(dim=-1)) / (rewards.std(dim=-1) + 1e-5)\n",
    "    ratio = torch.exp((new_logpro - old_logpro).mean(dim=-1))\n",
    "    loss = torch.min(ratio * rewards, torch.clamp(ratio, min=1-epsilon, max=1+epsilon) * rewards).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb19b1",
   "metadata": {},
   "source": [
    "DAPO\n",
    "\n",
    "$_{\\text{DAPO}}(\\theta) = \\mathbb{E}\\Big[ \\min\\big( r_i(\\theta) A_i,\\; \\text{clip}(r_i(\\theta),\\, 1-\\epsilon_{\\text{low}},\\, 1+\\epsilon_{\\text{high}})\\,A_i \\big) \\Big]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512eb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e3e3435",
   "metadata": {},
   "source": [
    "GBPO\n",
    "\n",
    "$\n",
    "\\mathcal{L}_{GBPO} = - \\ \\mathbb{E}{(s,a)} \\Bigg[ \\mathbf{1}[A \\neq 0] \\cdot\n",
    "\\frac{\\pi_\\theta(a|s)}{\n",
    "\\begin{cases}\n",
    "\\max(\\pi_{\\text{old}}(a|s), \\ \\text{sg}[\\pi_\\theta(a|s)]) & A \\ge 0 \\\\[6pt]\n",
    "\\max(\\pi_{\\text{old}}(a|s), \\ 1 - \\text{sg}[\\pi_\\theta(a|s)]) & A < 0\n",
    "\\end{cases}\n",
    "} \\cdot A \\Bigg]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_advantage(q: torch.Tensor, neg: torch.Tensor, tau_B: float = 0.75):\n",
    "    \"\"\"\n",
    "    q: 归一化偏好分（如时长分位数 in [0,1]）\n",
    "    neg: 是否点踩（bool）\n",
    "    规则：q>tau_B 且非点踩 -> +1；点踩 -> -1；其余 -> 0\n",
    "    \"\"\"\n",
    "    A = torch.zeros_like(q, dtype=torch.float)\n",
    "    A[(q > tau_B) & (~neg)] = 1.0\n",
    "    A[neg] = -1.0\n",
    "    return A\n",
    "\n",
    "def gbpo_loss_from_logits(\n",
    "    logits: torch.Tensor,         # [B, V]\n",
    "    actions: torch.Tensor,        # [B] 已选token/物品id\n",
    "    advantages: torch.Tensor,     # [B] in {-1, 0, +1}\n",
    "    old_logprobs: torch.Tensor | None = None,  # [B] 旧策略logP，可为None\n",
    "    eps: float = 1e-6,\n",
    "):\n",
    "    # 当前策略对 chosen action 的概率 πθ\n",
    "    logp_all = F.log_softmax(logits, dim=-1)          # [B, V]\n",
    "    logp_new = logp_all.gather(-1, actions.unsqueeze(-1)).squeeze(-1)  # [B]\n",
    "    pi_new = logp_new.exp().clamp_(eps, 1 - eps)      # [B]\n",
    "\n",
    "    # 旧策略概率；若没有，直接用 detach(πθ) 作为 π_old\n",
    "    if old_logprobs is None:\n",
    "        pi_old = pi_new.detach()\n",
    "    else:\n",
    "        pi_old = old_logprobs.exp().clamp_(eps, 1 - eps)\n",
    "\n",
    "    # 动态分母 π_old'（GBPO关键）\n",
    "    pos = (advantages >= 0)\n",
    "    neg = (advantages < 0)\n",
    "    denom = torch.empty_like(pi_new)\n",
    "    # 正样本：max(π_old, sg(π_new))\n",
    "    denom[pos] = torch.maximum(pi_old[pos], pi_new.detach()[pos])\n",
    "    # 负样本：max(π_old, 1 - sg(π_new))\n",
    "    denom[neg] = torch.maximum(pi_old[neg], 1.0 - pi_new.detach()[neg])\n",
    "\n",
    "    ratio = (pi_new / denom)\n",
    "    # 只对 A != 0 的样本回传\n",
    "    mask = advantages != 0\n",
    "    if mask.any():\n",
    "        loss = -(ratio[mask] * advantages[mask]).mean()\n",
    "    else:\n",
    "        loss = torch.zeros([], device=logits.device, dtype=logits.dtype)\n",
    "\n",
    "    stats = {\n",
    "        \"pi_new_mean\": pi_new.mean().item(),\n",
    "        \"denom_mean\": denom.mean().item(),\n",
    "        \"ratio_mean\": ratio[mask].mean().item() if mask.any() else float(\"nan\"),\n",
    "        \"pos_frac\": pos.float().mean().item(),\n",
    "        \"neg_frac\": neg.float().mean().item(),\n",
    "    }\n",
    "    return loss, stats\n",
    "\n",
    "# ---- 使用示例 ----\n",
    "B, V = 8, 1000\n",
    "logits   = torch.randn(B, V)              # 来自你的策略网络\n",
    "actions  = torch.randint(0, V, (B,))      # 这一步产生的选择\n",
    "q        = torch.rand(B)                  # 时长分位数（举例）\n",
    "neg_flag = torch.rand(B) < 0.2            # 20% 点踩（举例）\n",
    "A        = build_advantage(q, neg_flag, tau_B=0.75)\n",
    "\n",
    "loss, stats = gbpo_loss_from_logits(logits, actions, A, old_logprobs=None)\n",
    "loss.backward()\n",
    "print(\"GBPO loss:\", float(loss), stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860cb02",
   "metadata": {},
   "source": [
    "list-wise DPO\n",
    "\n",
    "$\\mathcal{L} = - \\mathbb{E} \\Bigg[ \\log \\sigma \\Big( \\log \\sum_{i_l \\in \\mathcal{I}l} \\exp(rw\\Delta \\, \\max(0, \\hat r_\\theta(x_u, i_w) - \\hat r_\\theta(x_u, i_l) - \\delta)) \\Big) + \\alpha \\log \\pi_\\theta(i_w|x_u) \\Bigg]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def listwise_dpo_loss(logprobs_w, logprobs_l, logprobs_ref_w, logprobs_ref_l,\n",
    "                      delta=0.0, alpha=0.1, beta=0.1):\n",
    "    \"\"\"\n",
    "    logprobs_w: [B]      模型对正样本的 log P\n",
    "    logprobs_l: [B, L]   模型对负样本的 log P\n",
    "    logprobs_ref_w: [B]  参考模型对正样本的 log P\n",
    "    logprobs_ref_l: [B, L] 参考模型对负样本的 log P\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算 reward：r = beta * (log pi - log pi_ref)\n",
    "    r_w = beta * (logprobs_w - logprobs_ref_w)             # [B]\n",
    "    r_l = beta * (logprobs_l - logprobs_ref_l)             # [B, L]\n",
    "\n",
    "    # margin: max(0, r_w - r_l - delta)\n",
    "    rw_delta = torch.clamp(r_w.unsqueeze(1) - r_l - delta, min=0)  # [B, L]\n",
    "\n",
    "    # list-wise aggregation: log sum exp\n",
    "    agg = torch.logsumexp(rw_delta, dim=1)   # [B]\n",
    "\n",
    "    # 主项：log σ(agg)\n",
    "    loss_main = -F.logsigmoid(agg).mean()\n",
    "\n",
    "    # 正则项：alpha * log pi_theta(i_w|x)\n",
    "    loss_reg = -alpha * logprobs_w.mean()\n",
    "\n",
    "    return loss_main + loss_reg\n",
    "\n",
    "\n",
    "# ========== 测试 ==========\n",
    "B, L = 2, 3\n",
    "logprobs_w     = torch.tensor([-1.0, -0.5])          # 模型正样本\n",
    "logprobs_l     = torch.tensor([[-2.0, -3.0, -1.5],\n",
    "                               [-1.0, -2.5, -2.0]]) # 模型负样本\n",
    "logprobs_ref_w = torch.tensor([-1.2, -0.6])          # ref正\n",
    "logprobs_ref_l = torch.tensor([[-2.2, -2.8, -1.6],\n",
    "                               [-1.1, -2.6, -2.1]]) # ref负\n",
    "\n",
    "loss = listwise_dpo_loss(logprobs_w, logprobs_l, logprobs_ref_w, logprobs_ref_l)\n",
    "print(\"List-wise DPO Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d00d1",
   "metadata": {},
   "source": [
    "ASPO: Asymmetric Importance Sampling Policy Optimization\n",
    "\n",
    "$r_i = \\frac{\\pi_{\\theta}(y_i| x) \\pi_{old}(y_i| x)}{sg(\\pi_{\\theta}^2(y_i| x))} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ff53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def aspo_loss(\n",
    "    logp,             # 当前策略 log πθ(a|s)\n",
    "    logp_old,         # 旧/参考策略 log π_ref(a|s)\n",
    "    advantages,       # A_t（可来自GRPO的group归一化）\n",
    "    eps_low=0.1, \n",
    "    eps_high=0.1, \n",
    "    kl_coef=0.0,      # 可选 KL 系数\n",
    "):\n",
    "    # 标准比率 r_std = π / π_old（用于 A<0，也用于hard-clip判定）\n",
    "    r_std = torch.exp(logp - logp_old)\n",
    "\n",
    "    # 倒置比率 r_flip = (π_old * π) / sg(π^2)  ⇔  exp(logp_old + logp - 2*sg(logp))\n",
    "    r_flip = torch.exp(logp_old + logp - 2.0 * logp.detach())\n",
    "\n",
    "    # Step 1: Hard clipping (token masking) —— 用标准比率判定并屏蔽极端样本\n",
    "    mask_hard = torch.zeros_like(advantages, dtype=torch.bool)\n",
    "    mask_hard |= (advantages < 0) & (r_std < (1.0 - eps_low))\n",
    "    mask_hard |= (advantages > 0) & (r_std > (1.0 + eps_high))\n",
    "\n",
    "    # 屏蔽后不参与学习（把 A 置 0）\n",
    "    adv_eff = advantages.masked_fill(mask_hard, 0.0)\n",
    "\n",
    "    # Step 2: 选择比率  ——  A>0 用倒置比率；A<0 用标准比率\n",
    "    r = torch.where(adv_eff > 0, r_flip, r_std)\n",
    "\n",
    "    # Step 3: Soft dual clipping —— 对 A>0 的 r 做软裁剪（只裁值、不裁梯度）\n",
    "    # 注意：detach 只用于权重，不影响 logp 的梯度\n",
    "    r_pos_soft = torch.clamp(r, 1.0 - eps_low, 1.0 + eps_high)\n",
    "    r_soft = torch.where(adv_eff > 0, r_pos_soft, r)          # 只对正优势启用soft-clip\n",
    "    weight = r_soft.detach()                                  # “只裁值不裁梯度”的关键\n",
    "\n",
    "    # 策略梯度形式：- weight * A * log πθ\n",
    "    pg_loss = -(weight * adv_eff * logp)\n",
    "\n",
    "    # 可选：KL 正则（和参考策略/旧策略）\n",
    "    kl = torch.exp(logp) * (logp - logp_old)  # per-token KL(π || π_old) 的近似项\n",
    "    loss = pg_loss + kl_coef * kl\n",
    "\n",
    "    # 归约（按需要也可先对序列做平均再 batch 平均）\n",
    "    return loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
