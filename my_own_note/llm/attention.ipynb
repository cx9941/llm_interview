{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef0f493",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "请问这个代码是否有误：\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        B, L, H = input_tensor.shape\n",
    "        K = self.k(input_tensor).reshape(B, L, self.num_heads, H // self.num_heads).permute(0, 2, 1, 3)\n",
    "        Q = self.q(input_tensor).reshape(B, L, self.num_heads, H // self.num_heads).permute(0, 2, 1, 3)\n",
    "        V = self.v(input_tensor).reshape(B, L, self.num_heads, H // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn_score =  F.softmax(Q @ K.transpose(2,3) / ((self.hidden_dim // self.num_heads) ** 0.5), dim=-1)\n",
    "        outputs = (attn_score @ V).transpose(1,2).reshape(B, L, H)\n",
    "        return outputs\n",
    "\n",
    "class MQA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k = nn.Linear(hidden_dim, hidden_dim // self.num_heads)\n",
    "        self.v = nn.Linear(hidden_dim, hidden_dim // self.num_heads)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        B, L, H = input_tensor.shape\n",
    "        Q = self.q(input_tensor).reshape(B, L, self.num_heads, H // self.num_heads).permute(0, 2, 1, 3)\n",
    "        K = self.k(input_tensor).unsqueeze(1)\n",
    "        V = self.v(input_tensor).unsqueeze(1)\n",
    "\n",
    "        attn_score =  F.softmax(Q @ K.transpose(2,3) / ((self.hidden_dim // self.num_heads) ** 0.5), dim=-1)\n",
    "        outputs = (attn_score @ V).transpose(1,2).reshape(B, L, H)\n",
    "        return outputs\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        assert num_heads % num_kv_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.group_dim = (hidden_dim // self.num_heads) * num_kv_heads\n",
    "\n",
    "        self.q = nn.Linear(hidden_dim, self.hidden_dim)\n",
    "        self.k = nn.Linear(hidden_dim, self.group_dim)\n",
    "        self.v = nn.Linear(hidden_dim, self.group_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        B, L, H = input_tensor.shape\n",
    "        Q = self.q(input_tensor).reshape(B, L, self.num_kv_heads, self.num_heads // self.num_kv_heads,  H // self.num_heads).permute(0, 2, 3, 1, 4) # B, group_num, head_num // group_num, L, H // head_num\n",
    "        K = self.k(input_tensor).reshape(B, L, 1, self.num_kv_heads, H // self.num_heads).permute(0, 3, 2, 1, 4)\n",
    "        V = self.v(input_tensor).reshape(B, L, 1, self.num_kv_heads, H // self.num_heads).permute(0, 3, 2, 1, 4)\n",
    "\n",
    "        attn_score =  F.softmax(Q @ K.transpose(-2,-1) / ((self.hidden_dim // self.num_heads) ** 0.5), dim=-1)\n",
    "        outputs = (attn_score @ V).permute(0, 3, 1, 2, 4).reshape(B, L, H)\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b0a48",
   "metadata": {},
   "source": [
    "自带KV cache的整合版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff07fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设有一层 Self-Attention\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, x, past_kv=None):\n",
    "        # x: [batch, seq_len, dim]\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            # 拼接历史缓存\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=1)\n",
    "            v = torch.cat([past_v, v], dim=1)\n",
    "\n",
    "        # 计算 Attention\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        attn_output = torch.matmul(F.softmax(attn_weights, dim=-1), v)\n",
    "\n",
    "        # 返回结果和新的缓存\n",
    "        return attn_output, (k, v)\n",
    "\n",
    "# 使用时：每次 forward 会返回新的缓存，供下一步复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5537bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-6.4724e-01,  3.3060e-01,  7.0604e-01,  ...,  1.2995e-01,\n",
       "           -3.8271e-02,  1.6505e-01],\n",
       "          [-4.8879e-02,  2.2648e-01,  3.0446e-01,  ...,  1.2712e-01,\n",
       "           -6.9186e-01,  8.4487e-02],\n",
       "          [ 1.2842e-01,  1.2270e-01,  3.8339e-01,  ...,  1.7273e-01,\n",
       "           -4.2586e-01,  4.5509e-02],\n",
       "          ...,\n",
       "          [-4.9807e-02,  9.0177e-04,  3.2556e-02,  ..., -1.9173e-02,\n",
       "            2.7522e-02, -2.4489e-02],\n",
       "          [-4.6460e-02,  1.7933e-04,  4.1181e-02,  ..., -1.8496e-02,\n",
       "            2.1442e-02, -1.6814e-02],\n",
       "          [-4.9998e-02, -6.3287e-03,  3.9826e-02,  ..., -1.0800e-02,\n",
       "            1.6482e-02, -2.4108e-02]],\n",
       " \n",
       "         [[ 3.8783e-01,  1.4279e-01, -2.3531e-02,  ...,  2.9379e-01,\n",
       "            2.1458e-01, -2.8641e-01],\n",
       "          [ 2.4096e-01,  3.3994e-01,  2.4054e-01,  ..., -2.4949e-02,\n",
       "            1.5015e-01, -5.4550e-02],\n",
       "          [ 2.7038e-01,  1.9394e-01,  1.2818e-01,  ..., -1.3305e-02,\n",
       "            1.6878e-01,  7.3579e-02],\n",
       "          ...,\n",
       "          [-1.8194e-02, -8.2657e-03,  1.9632e-02,  ..., -1.3464e-02,\n",
       "           -1.0587e-03,  2.6120e-02],\n",
       "          [-2.0602e-02, -1.0985e-02,  2.9434e-02,  ..., -1.1064e-02,\n",
       "            2.5591e-04,  1.6964e-02],\n",
       "          [-2.2006e-02, -1.2800e-02,  1.8876e-02,  ..., -1.3384e-02,\n",
       "           -1.7190e-03,  1.3871e-02]],\n",
       " \n",
       "         [[-1.9991e-01,  5.2414e-01, -2.0914e-01,  ..., -1.4470e-01,\n",
       "            4.0669e-01, -4.9465e-01],\n",
       "          [ 3.5611e-02,  5.3702e-01, -1.1627e-01,  ..., -1.6149e-01,\n",
       "            7.8964e-02, -3.2264e-01],\n",
       "          [ 1.8818e-01,  4.8470e-01, -1.8778e-01,  ..., -7.2245e-02,\n",
       "           -1.5984e-03,  2.0138e-02],\n",
       "          ...,\n",
       "          [-8.6772e-04, -1.2394e-02,  3.3060e-02,  ..., -1.0029e-02,\n",
       "           -1.2662e-03,  4.0556e-03],\n",
       "          [-1.6006e-02, -1.9396e-02,  2.9545e-02,  ..., -1.3707e-02,\n",
       "           -2.8987e-03,  6.8313e-03],\n",
       "          [-1.5365e-02, -1.8004e-03,  3.3727e-02,  ..., -1.7149e-02,\n",
       "           -3.5270e-03,  4.7698e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.0143e-01,  3.0813e-01,  5.0661e-01,  ...,  2.6079e-01,\n",
       "           -1.1385e-01,  3.0985e-02],\n",
       "          [ 1.2456e-02,  3.5494e-01,  1.6014e-01,  ...,  1.6776e-02,\n",
       "           -4.6037e-01,  1.4060e-01],\n",
       "          [ 2.0986e-01,  2.0332e-01,  1.8236e-01,  ...,  3.0843e-03,\n",
       "           -4.3241e-01,  2.1346e-01],\n",
       "          ...,\n",
       "          [-2.0473e-02, -6.2096e-03,  3.9339e-02,  ..., -4.9756e-02,\n",
       "           -3.5533e-03, -6.5166e-03],\n",
       "          [-1.9737e-02, -2.6218e-03,  3.5309e-02,  ..., -3.3850e-02,\n",
       "           -2.6160e-03, -7.9890e-03],\n",
       "          [-1.9567e-02,  2.5706e-03,  3.8336e-02,  ..., -4.5379e-02,\n",
       "           -4.7306e-03,  2.1237e-03]],\n",
       " \n",
       "         [[ 1.5608e-01,  4.6865e-02,  4.0464e-01,  ...,  1.3376e-01,\n",
       "            1.6169e-01,  5.1100e-01],\n",
       "          [ 2.4189e-01, -1.1323e-01, -6.0768e-02,  ...,  9.1382e-02,\n",
       "            9.6230e-02,  4.2986e-01],\n",
       "          [ 8.1223e-02, -1.2661e-01, -6.8532e-02,  ..., -9.0256e-02,\n",
       "            3.4674e-02,  2.1078e-01],\n",
       "          ...,\n",
       "          [-3.6759e-02, -1.9415e-02,  5.7649e-02,  ..., -2.8800e-02,\n",
       "            4.1584e-02,  1.0919e-02],\n",
       "          [-4.7791e-02, -2.4380e-02,  4.1737e-02,  ..., -2.7042e-02,\n",
       "            2.7956e-02,  6.8810e-03],\n",
       "          [-4.7737e-02, -1.9211e-02,  4.8325e-02,  ..., -2.2382e-02,\n",
       "            3.0619e-02,  1.0332e-02]],\n",
       " \n",
       "         [[-2.6846e-01,  2.8519e-01, -1.0501e-01,  ..., -1.3511e-01,\n",
       "            3.8666e-01,  6.6024e-03],\n",
       "          [-2.3970e-01,  5.8893e-02,  9.9470e-02,  ...,  2.2717e-02,\n",
       "            3.6522e-01,  1.0987e-01],\n",
       "          [-2.2171e-01,  8.2026e-03, -6.3239e-03,  ..., -2.4718e-01,\n",
       "            1.4111e-01, -7.6335e-02],\n",
       "          ...,\n",
       "          [-3.0598e-02, -4.1099e-02,  3.6163e-02,  ..., -2.3348e-02,\n",
       "            1.3199e-02, -4.5287e-03],\n",
       "          [-3.3211e-02, -4.2676e-02,  2.7336e-02,  ..., -2.3899e-02,\n",
       "            3.1241e-03,  1.0929e-03],\n",
       "          [-2.7032e-02, -3.5325e-02,  3.3319e-02,  ..., -2.0888e-02,\n",
       "            1.1903e-02, -6.8788e-03]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[-0.5398, -0.1531, -0.4026,  ...,  0.8004, -0.5827,  0.4919],\n",
       "          [-0.5070,  0.4732, -0.0060,  ...,  0.3363, -0.0746, -0.7705],\n",
       "          [-0.1934,  0.6541,  0.1458,  ...,  0.4431, -0.1782,  0.4139],\n",
       "          ...,\n",
       "          [-0.1892,  1.0221, -0.7881,  ...,  0.9706,  0.1550, -0.3609],\n",
       "          [-0.5537,  0.0652,  0.4246,  ..., -0.3453, -0.3042, -0.0117],\n",
       "          [ 0.2624,  0.0858,  0.1802,  ..., -0.1216, -0.5224,  0.0349]],\n",
       " \n",
       "         [[-0.0239,  0.4364,  0.5526,  ...,  0.2787, -0.3884,  0.3185],\n",
       "          [-0.3372,  0.5856, -0.3677,  ..., -0.3473, -0.2893,  0.1487],\n",
       "          [ 0.1845,  0.4326, -0.3877,  ..., -0.4465,  0.1124,  0.1552],\n",
       "          ...,\n",
       "          [ 0.0170, -0.1710, -0.9268,  ..., -0.0396, -0.5697, -0.1760],\n",
       "          [-0.0912, -0.2386,  0.3623,  ...,  0.2485, -0.1654,  0.3411],\n",
       "          [-0.2533, -0.4211,  0.1898,  ...,  0.3911,  0.0237,  0.4201]],\n",
       " \n",
       "         [[-0.9207,  0.5072, -0.3238,  ...,  0.8445,  0.6317, -0.2018],\n",
       "          [ 0.1653, -0.2794, -0.1065,  ..., -0.4281, -0.5920,  0.3106],\n",
       "          [ 0.5635,  0.5704,  0.6723,  ..., -0.0624,  0.4052, -0.3753],\n",
       "          ...,\n",
       "          [-0.2058,  0.3893,  0.1636,  ..., -0.4334,  0.5449, -1.1652],\n",
       "          [ 0.6920, -0.1104,  0.3635,  ..., -0.1628,  0.8519,  1.1123],\n",
       "          [ 0.2953,  0.3043,  0.0432,  ..., -0.5681, -0.1003,  0.1263]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.5595, -0.1075,  0.3019,  ..., -0.6039,  0.1442, -0.1372],\n",
       "          [-0.4976, -0.1247, -0.2413,  ...,  0.4589,  0.1055, -0.4037],\n",
       "          [ 0.2605,  0.1007, -0.1244,  ..., -0.2572,  0.3343, -0.7834],\n",
       "          ...,\n",
       "          [ 0.0090, -0.1630,  0.2392,  ...,  0.0405,  0.4111,  0.2677],\n",
       "          [ 0.4111,  0.6703,  0.1304,  ..., -0.3287,  0.3310,  0.1411],\n",
       "          [ 0.4690, -0.4557,  0.1140,  ...,  0.0816,  0.8517, -0.0704]],\n",
       " \n",
       "         [[ 0.3266,  0.8234,  0.4376,  ...,  0.3563,  0.1198, -0.1096],\n",
       "          [-0.7364, -0.0322,  0.5819,  ..., -0.1999,  0.3165,  0.2434],\n",
       "          [-0.8537,  0.7751,  0.0390,  ...,  0.3911,  0.4737, -0.2864],\n",
       "          ...,\n",
       "          [ 1.3595, -0.1352, -0.0125,  ...,  0.3482, -0.6593,  0.3068],\n",
       "          [-0.1540,  0.8620, -0.4324,  ...,  0.4357,  0.2404,  0.4016],\n",
       "          [-0.8086,  0.0022,  0.0668,  ..., -0.1269, -0.0239,  0.9465]],\n",
       " \n",
       "         [[ 1.2090,  0.1443,  0.3237,  ..., -0.3584,  0.3907, -0.7743],\n",
       "          [ 0.2874,  0.2167,  0.2400,  ..., -0.0998, -0.0754,  0.5684],\n",
       "          [-0.1117,  0.6787,  1.0341,  ..., -0.4800,  0.1460,  0.5792],\n",
       "          ...,\n",
       "          [-0.6727,  0.2637, -0.3796,  ..., -0.7513, -0.0160, -0.0176],\n",
       "          [ 0.3052, -0.9778,  0.4336,  ...,  0.3136, -0.1327,  0.1917],\n",
       "          [-0.0771,  0.5399,  0.0720,  ...,  0.5411, -0.2703,  0.1282]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[-0.0865, -0.8280,  0.2626,  ..., -0.1403,  0.8882,  1.4680],\n",
       "          [-0.0884,  0.1564, -0.2036,  ...,  0.2063,  0.9901,  0.1246],\n",
       "          [ 0.9605, -0.4165,  0.4430,  ..., -0.8043,  1.1751, -0.3952],\n",
       "          ...,\n",
       "          [ 0.1488, -0.4361, -0.6873,  ...,  0.5995, -0.7129, -0.4986],\n",
       "          [-0.2975,  0.8528,  0.3254,  ..., -0.3442, -0.6448, -0.0324],\n",
       "          [ 0.1512,  0.2998, -0.4086,  ..., -0.3682,  0.1254, -0.4889]],\n",
       " \n",
       "         [[-0.0918,  0.6345, -0.9569,  ...,  0.5049, -0.5224,  0.4805],\n",
       "          [ 0.7305,  1.1404, -0.8274,  ..., -0.0955, -0.0059, -0.3074],\n",
       "          [-0.4647, -0.3210,  0.2602,  ..., -0.0510,  0.1635,  0.3490],\n",
       "          ...,\n",
       "          [-0.0074, -0.0793,  0.3056,  ...,  1.0518, -0.0960,  0.3596],\n",
       "          [-0.5023,  0.1584, -0.0514,  ..., -1.1415,  0.3831,  1.2471],\n",
       "          [ 0.4684,  0.2322,  0.0656,  ...,  0.2346, -0.4249, -0.2239]],\n",
       " \n",
       "         [[-0.1927,  0.0022,  0.9129,  ..., -0.2267, -0.8993,  0.8639],\n",
       "          [-0.3452,  0.4015, -0.2488,  ..., -1.0033,  0.1347, -0.8297],\n",
       "          [ 0.5332, -1.0055, -0.2079,  ..., -0.6077,  0.7250, -0.1552],\n",
       "          ...,\n",
       "          [ 0.9259, -0.0452, -0.4233,  ..., -0.2561, -0.3200,  0.2306],\n",
       "          [-0.6782, -0.4017, -0.2685,  ..., -0.0370,  1.0207,  0.7801],\n",
       "          [ 0.8740, -0.0993, -0.4776,  ...,  0.1627, -0.9062, -0.6771]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.4997, -0.2031, -0.4103,  ..., -0.8684,  0.4688, -0.2393],\n",
       "          [-1.3700,  0.6379, -0.2689,  ..., -0.4022,  0.8489,  1.1100],\n",
       "          [ 0.3818, -0.0761,  0.5441,  ..., -0.1027,  0.2499,  0.4201],\n",
       "          ...,\n",
       "          [-0.5613,  0.2107, -1.1077,  ...,  0.3522,  0.6870, -0.2063],\n",
       "          [-0.6420,  0.5195,  0.2196,  ...,  0.6952, -0.9693,  0.1435],\n",
       "          [-0.7645, -0.0737,  0.9115,  ...,  1.0860,  0.2773,  0.4723]],\n",
       " \n",
       "         [[-0.7600, -0.6242,  0.7888,  ..., -0.2081,  0.8231,  0.7618],\n",
       "          [ 0.6191,  0.3592,  0.2115,  ..., -0.5534,  0.6105,  0.5405],\n",
       "          [-0.0015, -0.3517, -0.1410,  ..., -0.7995,  0.3977, -0.1124],\n",
       "          ...,\n",
       "          [ 0.1450, -0.2947,  0.1562,  ...,  0.7859, -0.3585,  1.0292],\n",
       "          [-0.7435,  1.2685,  0.0407,  ...,  0.5063, -1.3622, -0.3111],\n",
       "          [ 1.0336,  1.0218, -0.1311,  ..., -1.4169, -0.3014, -0.0379]],\n",
       " \n",
       "         [[-0.4312,  0.2302,  0.2029,  ...,  0.2412, -0.4152,  0.9172],\n",
       "          [ 0.5464, -0.2103,  1.2150,  ..., -0.1370, -0.4436,  1.2645],\n",
       "          [ 0.4195,  0.2520,  0.3346,  ...,  0.0719,  1.4375,  0.2604],\n",
       "          ...,\n",
       "          [ 0.3691, -0.3172,  0.1361,  ..., -0.5457, -0.1298,  0.2134],\n",
       "          [ 0.8217, -0.1487,  0.6304,  ..., -0.2959, -0.3016,  0.0477],\n",
       "          [ 0.5886,  0.2427,  0.4647,  ..., -0.2365, -0.8864,  0.7638]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, hidden_dim, group_num, head_num):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.group_num = group_num\n",
    "        self.head_num = head_num\n",
    "        self.w_q = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.w_k = nn.Linear(self.hidden_dim, self.hidden_dim // (head_num // group_num))\n",
    "        self.w_v = nn.Linear(self.hidden_dim, self.hidden_dim // (head_num // group_num))\n",
    "        self.o = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "    \n",
    "    def forward(self, q, kv=None, k_cache=None, v_cache=None, attn_mask=None):\n",
    "        if kv is None:\n",
    "            kv = q\n",
    "    \n",
    "        B, Lq, H = q.shape\n",
    "        B, Lkv, H = kv.shape\n",
    "\n",
    "        Q = self.w_q(q)\n",
    "        K = self.w_k(kv)\n",
    "        V = self.w_v(kv)\n",
    "\n",
    "        if k_cache:\n",
    "            k_cache = torch.concat([k_cache, K], dim=1)\n",
    "            Lkv = k_cache.shape[1]\n",
    "        else:\n",
    "            k_cache = K\n",
    "\n",
    "        if v_cache:\n",
    "            v_cache = torch.concat([v_cache, V], dim=1)\n",
    "            Lkv = v_cache.shape[1]\n",
    "        else:\n",
    "            v_cache = V\n",
    "\n",
    "        Q = Q.reshape(B, Lq, self.group_num, self.head_num // self.group_num, H // self.head_num).permute(0, 2, 3, 1, 4) # bgplh\n",
    "        K = k_cache.reshape(B, Lkv, self.group_num, H // self.head_num).permute(0, 2, 1, 3) # bgth\n",
    "        V = v_cache.reshape(B, Lkv, self.group_num, H // self.head_num).permute(0, 2, 1, 3) # bgth\n",
    "\n",
    "        attn = torch.einsum(\"bgplh, bgth->bgplt\", Q, K) / ((self.hidden_dim // self.head_num * self.group_num) ** 0.5) # bgplt\n",
    "\n",
    "        tril_mask = torch.ones(1, 1, 1, Lq, Lkv).tril()\n",
    "        attn = attn.masked_fill(tril_mask==0, -float('inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        outputs = torch.einsum(\"bgplt, bgth->bgplh\", attn, V) # bgplh\n",
    "        outputs = outputs.permute(0,3,1,2,4).reshape(B, Lq, H)\n",
    "        outputs = self.o(outputs)\n",
    "        return outputs, k_cache, v_cache\n",
    "    \n",
    "q = torch.randn(10, 512, 768)\n",
    "gpa = GQA(hidden_dim=768, group_num=2, head_num=4)\n",
    "gpa(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
