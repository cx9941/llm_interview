{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a026f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/chenxi/miniconda3/envs/py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerBase\n",
    "\n",
    "model_name = '../pretrained_moels/Qwen2.5-0.5B'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.eos_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c34d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['prompt'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.data['prompt'][idx]\n",
    "        response = self.data['response'][idx]\n",
    "        context = prompt + response\n",
    "        \n",
    "        prompt_ids = self.tokenizer(prompt, truncation=True, max_length=self.max_len)\n",
    "        prompt_ids['token_type_ids'] = [0 for i in range(len(prompt_ids['input_ids']))]\n",
    "        \n",
    "        response_ids = self.tokenizer(response, truncation=True, max_length=self.max_len)\n",
    "        response_ids['token_type_ids'] = [1 for i in range(len(response_ids['input_ids']))]\n",
    "\n",
    "        input_ids = {}\n",
    "        for key in prompt_ids:\n",
    "            input_ids[key] = prompt_ids[key] + response_ids[key]\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    def __call__(self, examples):\n",
    "        batch = self.tokenizer.pad(examples, padding=True, return_tensors='pt')\n",
    "        batch['labels'] = batch['input_ids']\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollator(tokenizer=tokenizer)\n",
    "\n",
    "train_data = {\n",
    "    \"prompt\": [\n",
    "        \"Tell me about the capital of France.\",\n",
    "        \"Translate 'good morning' into Spanish.\",\n",
    "        \"What is the boiling point of water in Celsius?\",\n",
    "        \"Write a short greeting to a new customer.\",\n",
    "        \"Who is the author of 'Pride and Prejudice'?\",\n",
    "        \"Explain what machine learning is in one sentence.\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"In Spanish, 'good morning' is 'buenos días'.\",\n",
    "        \"The boiling point of water is 100 degrees Celsius at standard pressure.\",\n",
    "        \"Welcome to our store! We are glad to have you here.\",\n",
    "        \"The author is Jane Austen.\",\n",
    "        \"Machine learning is a field of AI where computers learn from data to make predictions or decisions.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "train_dataset = SFTDataset(train_data, tokenizer, max_len=512)\n",
    "train_dataloder = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "\n",
    "def cross_entropy_loss_manual(logits, targets, ignore_index=None):\n",
    "    \"\"\"\n",
    "    logits: [N, C]  float\n",
    "    targets: [N]  long\n",
    "    \"\"\"\n",
    "    # 1. 对 logits 做 log_softmax\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # [N, C]\n",
    "\n",
    "    # 2. 取出标签对应的 log_prob\n",
    "    if ignore_index is not None:\n",
    "        mask = targets != ignore_index\n",
    "        log_probs = log_probs[mask]\n",
    "        targets = targets[mask]\n",
    "\n",
    "    nll_loss = -log_probs[torch.arange(targets.size(0)), targets]\n",
    "\n",
    "    # 3. 取平均\n",
    "    return nll_loss.mean()\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optim = Adam(model.parameters(), weight_decay=0.99, lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloder:\n",
    "    outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    input_ids = batch['input_ids']\n",
    "    labels = batch['labels']\n",
    "    token_type_ids = batch['token_type_ids']\n",
    "\n",
    "    shift_labels = labels.clone()\n",
    "    shift_labels[token_type_ids==0] = -100\n",
    "\n",
    "    shift_labels = shift_labels[:, 1:].reshape(-1).contiguous()\n",
    "    shift_logits = logits[:, :-1].reshape(-1, logits.shape[-1]).contiguous()\n",
    "\n",
    "    loss = cross_entropy_loss_manual(shift_logits, shift_labels, ignore_index=-100)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.zero_grad()\n",
    "    optim.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
