{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4551bb01",
   "metadata": {},
   "source": [
    "写一下L1、L2正则化公式，分别适用于什么场景，从数学角度讲一下两者的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d3fc5",
   "metadata": {},
   "source": [
    "# L1 与 L2 正则化\n",
    "\n",
    "- **公式：**  \n",
    "  - L1 正则化：  \n",
    "    $$\n",
    "    \\mathcal{L}_{L1} = \\mathcal{L} + \\lambda \\sum |w_i|\n",
    "    $$\n",
    "  - L2 正则化：  \n",
    "    $$\n",
    "    \\mathcal{L}_{L2} = \\mathcal{L} + \\lambda \\sum w_i^2\n",
    "    $$\n",
    "\n",
    "- **适用场景：**  \n",
    "  - L1：适合高维稀疏场景，用于特征选择，会产生稀疏解。  \n",
    "  - L2：适合大部分特征都有用的场景，用于防止过拟合，让模型更稳定。  \n",
    "\n",
    "- **数学区别：**  \n",
    "  - L1 的约束区域是菱形，带有尖角，解更容易落在坐标轴上 → 某些参数为 0 → 稀疏。  \n",
    "  - L2 的约束区域是圆形/球体，光滑无尖点，解会均匀收缩但不为 0 → 平滑。  \n",
    "\n",
    "---\n",
    "\n",
    "📌 **一句话总结：**  \n",
    "L1 产生稀疏解（特征选择），L2 产生平滑解（防过拟合，稳定性好）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a28090",
   "metadata": {},
   "source": [
    "写一下逻辑回归的表达式，参数怎么推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4cce0",
   "metadata": {},
   "source": [
    "# 逻辑回归\n",
    "\n",
    "## 1. 模型表达式\n",
    "\n",
    "逻辑回归用于二分类问题，预测概率为：\n",
    "\n",
    "$$\n",
    "P(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)\n",
    "= \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^d$：特征向量  \n",
    "- $\\mathbf{w}$：参数向量  \n",
    "- $b$：偏置  \n",
    "- $\\sigma(z)$：Sigmoid 函数  \n",
    "\n",
    "同时：\n",
    "$$\n",
    "P(y=0 \\mid \\mathbf{x}) = 1 - P(y=1 \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 参数估计（极大似然）\n",
    "\n",
    "训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，似然函数为：\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) = \\prod_{i=1}^n P(y_i \\mid \\mathbf{x}_i)\n",
    "= \\prod_{i=1}^n \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)^{y_i} \\, \\big(1-\\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\big)^{1-y_i}\n",
    "$$\n",
    "\n",
    "取对数得到对数似然函数：\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\Big[ y_i \\log \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) + (1-y_i)\\log(1-\\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)) \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 参数推导（梯度）\n",
    "\n",
    "Sigmoid 的导数：\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1-\\sigma(z))\n",
    "$$\n",
    "\n",
    "对参数求偏导：\n",
    "\n",
    "- 对 $\\mathbf{w}$：\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\mathbf{w}} \n",
    "= \\sum_{i=1}^n \\big(y_i - \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\big)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "- 对 $b$：\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial b} \n",
    "= \\sum_{i=1}^n \\big(y_i - \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 参数解法\n",
    "\n",
    "逻辑回归没有解析解，因为对数似然是非线性的。  \n",
    "一般采用数值优化方法（梯度下降 / 牛顿法）：\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_{i=1}^n (y_i - \\hat{y}_i)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "其中 $\\hat{y}_i = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)$，$\\eta$ 为学习率。\n",
    "\n",
    "---\n",
    "\n",
    "✅ **总结：**  \n",
    "逻辑回归通过 Sigmoid 将线性模型转为概率输出，参数用极大似然估计，最后通过迭代优化求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6292c",
   "metadata": {},
   "source": [
    "# Dropout 在训练与测试阶段的区别及差异平衡\n",
    "\n",
    "## 训练阶段\n",
    "- **随机屏蔽神经元**：以概率 \\(p\\) 将一部分 hidden states 置为 0。  \n",
    "- **缩放保留神经元**：为了保证期望不变，保留的神经元除以 \\(1-p\\)。  \n",
    "公式如下：  \n",
    "$h^{(train)} = \\frac{m \\odot h}{1-p}, \\quad m \\sim \\text{Bernoulli}(1-p)$\n",
    "其中：\n",
    "- \\(h\\)：隐层输出（hidden states）\n",
    "- \\(m\\)：随机 mask 向量\n",
    "- \\(p\\)：dropout rate\n",
    "\n",
    "这样可以看作训练时在不同的“子网络”上进行参数更新，达到正则化效果。\n",
    "\n",
    "---\n",
    "\n",
    "## 测试阶段\n",
    "- **不再随机屏蔽**：推理需要稳定性，因此不再对神经元进行随机丢弃。  \n",
    "- **直接使用全量神经元**：由于训练时做了缩放补偿，测试时可直接使用完整的 \\(h\\)，分布保持一致。  \n",
    "公式：\n",
    "$h^{(test)} = h$\n",
    "\n",
    "---\n",
    "\n",
    "## 如何平衡训练和测试的差异\n",
    "1. **缩放补偿**：训练阶段对保留单元除以 \\(1-p\\)，保证期望一致。  \n",
    "2. **蒙特卡洛 Dropout**：推理时继续使用 dropout，多次采样前向传播，用于不确定性估计。  \n",
    "3. **合理选择 dropout rate**：一般在 0.1–0.5 之间，过大会导致 gap 过大。  \n",
    "4. **与 BatchNorm 搭配注意**：通常先做 BN，再做 Dropout，以避免统计量偏差。\n",
    "\n",
    "---\n",
    "\n",
    "## 面试简洁回答\n",
    "> Dropout 在训练阶段会随机屏蔽神经元并缩放保留单元；测试阶段则使用全量神经元不再屏蔽。通过缩放补偿，使训练和测试的分布保持一致，从而平衡差异。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
